{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d65c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ege/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(detected_labels_and_boxes_result(model_yolov8,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ege/Desktop/test_images/3/right.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 153\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m#image_captioning_result(cv2.imread(\"C:/Users/ege/Desktop/test_images/3/right.jpg\"))\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     initialize_models()\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(detected_labels_and_boxes_result(model_yolov8,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ege/Desktop/test_images/3/right.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m, in \u001b[0;36minitialize_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m model_wotr \u001b[38;5;241m=\u001b[39m YOLO(wotr_path)\n\u001b[0;32m     38\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMiDaS_small\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m---> 39\u001b[0m model_midas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintel-isl/MiDaS\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_type)\n\u001b[0;32m     40\u001b[0m device_midas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m model_midas\u001b[38;5;241m.\u001b[39mto(device_midas)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\hub.py:592\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _add_to_sys_path(hubconf_dir):\n\u001b[0;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[1;32m--> 592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[0;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\hub.py:106\u001b[0m, in \u001b[0;36m_import_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    104\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, Loader)\n\u001b[1;32m--> 106\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\hubconf.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdpt_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPTDepthModel\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidas_net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MidasNet\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmidas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidas_net_custom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MidasNet_small\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\dpt_depth.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     FeatureFusionBlock_custom,\n\u001b[0;32m      7\u001b[0m     Interpolate,\n\u001b[0;32m      8\u001b[0m     _make_encoder,\n\u001b[0;32m      9\u001b[0m     forward_beit,\n\u001b[0;32m     10\u001b[0m     forward_swin,\n\u001b[0;32m     11\u001b[0m     forward_levit,\n\u001b[0;32m     12\u001b[0m     forward_vit,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlevit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stem_b4_transpose\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_act_layer\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\blocks.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     _make_pretrained_beitl16_512,\n\u001b[0;32m      6\u001b[0m     _make_pretrained_beitl16_384,\n\u001b[0;32m      7\u001b[0m     _make_pretrained_beitb16_384,\n\u001b[0;32m      8\u001b[0m     forward_beit,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mswin_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     forward_swin,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mswin2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     _make_pretrained_swin2l24_384,\n\u001b[0;32m     15\u001b[0m     _make_pretrained_swin2b24_384,\n\u001b[0;32m     16\u001b[0m     _make_pretrained_swin2t16_256,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\intel-isl_MiDaS_master\\midas\\backbones\\beit.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import dist_measurement\n",
    "\n",
    "yolov8_path = '../../models/object-detection/v1/yolov8m.pt'\n",
    "wotr_path = '../../models/object-detection/v3/best_v3.pt'\n",
    "\n",
    "# model should pay attention to these class indexes\n",
    "wotr_class_pruned_indexes = [0, 1, 2, 3, 4, 12, 13, 14, 15, 16, 20, 21]\n",
    "\n",
    "# for these indexes, we may not measure distance\n",
    "wotr_class_important_indexes = [3, 4, 20, 21]\n",
    "\n",
    "model_yolov8 = None\n",
    "model_wotr = None\n",
    "model_midas = None\n",
    "model_captioning = None\n",
    "transform_midas = None\n",
    "device_midas = None\n",
    "\n",
    "def initialize_models():\n",
    "    global model_yolov8\n",
    "    global model_wotr\n",
    "    global model_midas\n",
    "    global model_captioning\n",
    "    global transform_midas\n",
    "    global device_midas\n",
    "\n",
    "    model_yolov8 = YOLO(yolov8_path)\n",
    "    \n",
    "    model_wotr = YOLO(wotr_path)\n",
    "    \"\"\"\n",
    "    model_type = \"MiDaS_small\" \n",
    "    model_midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "    device_midas = torch.device(\"mps\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model_midas.to(device_midas)\n",
    "    model_midas.eval()\n",
    "    \n",
    "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "    if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "        transform_midas = midas_transforms.dpt_transform\n",
    "    else:\n",
    "        transform_midas = midas_transforms.small_transform\n",
    "    \"\"\"\n",
    "    model_captioning = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "def detected_labels_and_boxes_result(model, image):\n",
    "    results = model.predict(image)\n",
    "    result = results[0]\n",
    "    return {'names': result.names, \"classes\": result.boxes.cls, \"boxes\": result.boxes.xyxy, \"confs\": result.boxes.conf}\n",
    "\n",
    "def depth_map_result(image):\n",
    "    if isinstance(image, str):\n",
    "        input_image = cv2.imread(image)\n",
    "    else:\n",
    "        input_image = image\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "    input_image = transform_midas(input_image).to(device_midas)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth_prediction = model_midas(input_image)\n",
    "\n",
    "    depth_map = depth_prediction.squeeze().cpu().numpy()\n",
    "\n",
    "    depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255\n",
    "    depth_map = depth_map.astype(np.uint8)\n",
    "    return depth_map\n",
    "\n",
    "def image_captioning_result(image):\n",
    "    if not isinstance(image, str):\n",
    "        image = Image.fromarray(np.uint8(image)).convert('RGB')\n",
    "    return model_captioning(image)\n",
    "\n",
    "def stereo_vision_distance_result(image_left, image_right, labels_boxes_json):\n",
    "    classes = labels_boxes_json[\"classes\"]\n",
    "    boxes = labels_boxes_json[\"boxes\"]\n",
    "    # TODO \n",
    "    # returns distances dict for each boxes\n",
    "    # example output \n",
    "    # {'names': result.names, \"classes\": result.boxes.cls, \"boxes\": result.boxes.xyxy, \"confs\": result.boxes.conf, \"distances\": distances}\n",
    "    fl = 75-246.3441162109375*155/490.97529220581055\n",
    "    image_left = cv2.cvtColor(image_left, cv2.COLOR_BGR2RGB)\n",
    "    image_right = cv2.cvtColor(image_right, cv2.COLOR_BGR2RGB)\n",
    "    sz1 = image_right.shape[1]\n",
    "    sz2 = image_right.shape[0]\n",
    "    det = labels_boxes_json.boxes\n",
    "    lbls = labels_boxes_json.classes\n",
    "    centre = sz1/2\n",
    "    def get_dist_to_centre_tl(box, cntr = centre):\n",
    "        pnts = np.array(dist_measurement.tlbr_to_corner(box))[:,0]\n",
    "        return abs(pnts - centre)\n",
    "    \n",
    "    def get_dist_to_centre_br(box, cntr = centre):\n",
    "        pnts = np.array(dist_measurement.tlbr_to_corner_br(box))[:,0]\n",
    "        return abs(pnts - centre)\n",
    "    \n",
    "    dists_tl =  dist_measurement.get_horiz_dist_corner_tl(det)\n",
    "    dists_br =  dist_measurement.get_horiz_dist_corner_br(det)\n",
    "    final_dists = []\n",
    "    dctl = dist_measurement.get_dist_to_centre_tl(det[0])\n",
    "    dcbr = dist_measurement.get_dist_to_centre_br(det[0])\n",
    "    for i, j in zip(*tracks):\n",
    "        if dctl[i] < dcbr[i]:\n",
    "            final_dists.append((dists_tl[i][j],lbls[i]))\n",
    "        else:\n",
    "            final_dists.append((dists_br[i][j],lbls[i]))\n",
    "        \n",
    "        \n",
    "    tantheta = (1/(155-fl))*(59.0/2)*sz1/246.3441162109375\n",
    "    fd = [i for (i,j) in final_dists]\n",
    "    dists_away = (59.0/2)*sz1*(1/tantheta)/np.array(fd)+fl\n",
    "    cat_dist = []\n",
    "    for i in range(len(dists_away)):\n",
    "        cat_dist.append(f'{lbls[i]} {dists_away[i]:.1f}cm')\n",
    "        print(\"Estimation:\")\n",
    "        print(f'{lbls[i]} is {dists_away[i]:.1f}cm away')\n",
    "          \n",
    "\n",
    "# example usage: get_model_output_from_camera(image_captioning_result, printable=True)\n",
    "def get_model_output_from_camera(model_method, show=False, printable=False):\n",
    "    cap = cv2.VideoCapture(0) \n",
    "    while True:\n",
    "        ret, frame = cap.read()  \n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        result = model_method(frame)\n",
    "        if printable:\n",
    "            print(result)\n",
    "        if show:\n",
    "            cv2.imshow(\"output\", result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def get_model_output_from_frame(model_method, frame ,show=False, printable=False):\n",
    "    result = model_method(frame)\n",
    "    if printable:\n",
    "        print(result)\n",
    "    if show:\n",
    "        cv2.imshow(\"output\", result)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    #image_captioning_result(cv2.imread(\"C:/Users/ege/Desktop/test_images/3/right.jpg\"))\n",
    "    initialize_models()\n",
    "    print(detected_labels_and_boxes_result(model_yolov8,\"C:/Users/ege/Desktop/test_images/3/right.jpg\"))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aab601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
