{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab5206ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ege/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ege/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in C:\\Users\\ege/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "\n",
    "import dist_measurement\n",
    "import sys\n",
    "sys.path.insert(1, '../server/')\n",
    "import evaluation\n",
    "\n",
    "evaluation.initialize_models()\n",
    "case_1_dist = 150.0 #in cms\n",
    "case_2_dist = 120.0 #in cms\n",
    "dist_btw_cameras = 4.8 #in cms\n",
    "total_pixels = 0 #sz1\n",
    "object_pixels_case1 = 0\n",
    "object_pixels_case2 = 0\n",
    "calib_case1_img_left_path = \"../../documents/calibration_images/5/left1.jpg\"\n",
    "calib_case1_img_right_path = \"../../documents/calibration_images/5/right1.jpg\"\n",
    "calib_case2_img_left_path = \"../../documents/calibration_images/6/left2.jpg\"\n",
    "calib_case2_img_right_path = \"../../documents/calibration_images/6/right2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4512617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_info(image_left, image_right, labels_boxes_json_left, labels_boxes_json_right):\n",
    "    image_left = cv2.cvtColor(image_left, cv2.COLOR_BGR2RGB)\n",
    "    image_right = cv2.cvtColor(image_right, cv2.COLOR_BGR2RGB)\n",
    "    sz1 = image_right.shape[1]\n",
    "    sz2 = image_right.shape[0]\n",
    "    det = [labels_boxes_json_left[\"boxes\"], labels_boxes_json_right[\"boxes\"]]\n",
    "    lbls = [labels_boxes_json_left[\"classes\"], labels_boxes_json_right[\"classes\"]]\n",
    "    centre = sz1/2\n",
    "    def get_dist_to_centre_tl(box, cntr = centre):\n",
    "        pnts = np.array(dist_measurement.tlbr_to_corner(box))[:,0]\n",
    "        return abs(pnts - centre)\n",
    "    \n",
    "    def get_dist_to_centre_br(box, cntr = centre):\n",
    "        pnts = np.array(dist_measurement.tlbr_to_corner_br(box))[:,0]\n",
    "        return abs(pnts - centre)\n",
    "    cost = dist_measurement.get_cost(det, lbls = lbls)\n",
    "    tracks = scipy.optimize.linear_sum_assignment(cost)\n",
    "    \n",
    "    dists_tl =  dist_measurement.get_horiz_dist_corner_tl(det)\n",
    "    dists_br =  dist_measurement.get_horiz_dist_corner_br(det)\n",
    "    final_dists = []\n",
    "    dctl = get_dist_to_centre_tl(det[0])\n",
    "    dcbr = get_dist_to_centre_br(det[0])\n",
    "    for i, j in zip(*tracks):\n",
    "        if len(lbls) <= i:\n",
    "            continue\n",
    "        if dctl[i] < dcbr[i]:\n",
    "            final_dists.append((dists_tl[i][j],lbls[i]))\n",
    "        else:\n",
    "            final_dists.append((dists_br[i][j],lbls[i]))\n",
    "    fd = [i for (i,j) in final_dists]\n",
    "    return sz1, fd[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae64f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ege\\Smart-Glasses-for-Blind-People\\scripts\\utils\\..\\..\\documents\\calibration_images\\5\\left1.jpg: 480x640 1 apple, 2 refrigerators, 499.5ms\n",
      "Speed: 0.0ms preprocess, 499.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\ege\\Smart-Glasses-for-Blind-People\\scripts\\utils\\..\\..\\documents\\calibration_images\\5\\right1.jpg: 480x640 1 apple, 2 refrigerators, 421.8ms\n",
      "Speed: 0.9ms preprocess, 421.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels:  640\n",
      "Object pixels on case 1:  28.04599\n"
     ]
    }
   ],
   "source": [
    "total_pixels, object_pixels_case1 = get_pixel_info(cv2.imread(calib_case1_img_left_path), \n",
    "                              cv2.imread(calib_case1_img_right_path), \n",
    "                              evaluation.detected_labels_and_boxes_result(evaluation.model_yolov8,calib_case1_img_left_path), \n",
    "                              evaluation.detected_labels_and_boxes_result(evaluation.model_yolov8,calib_case1_img_right_path))\n",
    "print(\"Total pixels: \", total_pixels)\n",
    "print(\"Object pixels on case 1: \", object_pixels_case1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5522bd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ege\\Smart-Glasses-for-Blind-People\\scripts\\utils\\..\\..\\documents\\calibration_images\\6\\left2.jpg: 480x640 1 apple, 2 refrigerators, 460.9ms\n",
      "Speed: 2.8ms preprocess, 460.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\ege\\Smart-Glasses-for-Blind-People\\scripts\\utils\\..\\..\\documents\\calibration_images\\6\\right2.jpg: 480x640 1 apple, 2 refrigerators, 453.4ms\n",
      "Speed: 0.0ms preprocess, 453.4ms inference, 10.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pixels:  640\n",
      "Object pixels on case 2:  34.43347\n"
     ]
    }
   ],
   "source": [
    "total_pixels, object_pixels_case2 = get_pixel_info(cv2.imread(calib_case2_img_left_path), \n",
    "                              cv2.imread(calib_case2_img_right_path), \n",
    "                              evaluation.detected_labels_and_boxes_result(evaluation.model_yolov8,calib_case2_img_left_path), \n",
    "                              evaluation.detected_labels_and_boxes_result(evaluation.model_yolov8,calib_case2_img_right_path))\n",
    "print(\"Total pixels: \", total_pixels)\n",
    "print(\"Object pixels on case 2: \", object_pixels_case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "683552ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = -11.7232268698777\n",
      "tantheta = 0.338647631699787\n"
     ]
    }
   ],
   "source": [
    "f, tantheta = symbols('f tantheta')\n",
    "equation1 = Eq(case_1_dist, f + (total_pixels * dist_btw_cameras) / (2 * object_pixels_case1 * tantheta))\n",
    "equation2 = Eq(case_2_dist, f + (total_pixels * dist_btw_cameras) / (2 * object_pixels_case2 * tantheta))\n",
    "solution = solve((equation1, equation2), (f, tantheta))\n",
    "print(\"f =\", solution[0][0])\n",
    "print(\"tantheta =\", solution[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9d249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b52f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
